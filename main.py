from dotenv import load_dotenv
from flask import Flask, request, jsonify
from flask_cors import CORS
from openai import OpenAI
import chromadb
from sentence_transformers import SentenceTransformer
import json
import os
import re

os.environ["TOKENIZERS_PARALLELISM"] = "false"

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
api_key = "api_key"

# Flask ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸°í™”
app = Flask(__name__)
CORS(app)
client = OpenAI(api_key=api_key)

with open("sami_prompt.txt", "r", encoding="utf-8") as file:
    sami_prompt = file.read()

# Hugging Face ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# ChromaDB ì´ˆê¸°í™” (Persistent ëª¨ë“œ)
chroma_client = chromadb.PersistentClient(path="./chroma_db")

# ë°ì´í„° ìœ í˜•ë³„ ì»¬ë ‰ì…˜ ìƒì„±
scholarship_collection = chroma_client.get_or_create_collection(name="scholarship_knowledge")
haksa_collection = chroma_client.get_or_create_collection(name="haksa_knowledge")
major_collection = chroma_client.get_or_create_collection(name="major_knowledge")
ePortfolio_collection = chroma_client.get_or_create_collection(name="ePortfolio_knowledge")
facility_collection = chroma_client.get_or_create_collection(name="facility_knowledge")
student_collection = chroma_client.get_or_create_collection(name="student_knowledge")
double_major_collection = chroma_client.get_or_create_collection(name="double_major_knowledge")
graduation_collection = chroma_client.get_or_create_collection(name="graduation_knowledge")
service_collection = chroma_client.get_or_create_collection(name="service_knowledge")
introduction_collection = chroma_client.get_or_create_collection(name="introduction_knowledge")
school_collection = chroma_client.get_or_create_collection(name="school_knowledge")

def process_scholarship_data():
    with open("Training_Data/scholarship.json", "r", encoding="utf-8") as file:
        scholarship_data = json.load(file)

    documents = []
    for item in scholarship_data:
        documents.append(
            f"{item['name']} - ëŒ€ìƒ: {item['eligibility']} / ì§€ì› ê¸ˆì•¡: {item['amount']} / ì‹ ì²­ ë°©ë²•: {item['application']}")
    return documents

def process_graduation_data():
    with open("Training_Data/graduation.json", "r", encoding="utf-8") as file:
        graduation_data = json.load(file)

    documents = []
    for item in graduation_data:
        documents.append(
            f"{item['name']} - í•™ì : {item['grade']} / í•™ê¸°: {item['semester']} / í‰ì  í‰ê· : {item['GPA']} / ì „í™”ë²ˆí˜¸: {item['phoneNumber']} / {item['note']} / í™ˆí˜ì´ì§€: {item['page']}")
    return documents

def process_service_data():
    with open("Training_Data/service.json", "r", encoding="utf-8") as file:
        service_data = json.load(file)

    documents = []
    for item in service_data:
        text = f"{item['title']} / í™ˆí˜ì´ì§€: {item['page']}"
        if item.get("phoneNumber"):
            text += f" / ì „í™”ë²ˆí˜¸: {item['phoneNumber']}"
        if item.get("category"):
            text += f" / ì¢…ë¥˜: {item['category']}"
        if item.get("method"):
            text += f" / ë°©ë²•: {item['method']}"
        if item.get("Training_Center"):
            text += f" / í›ˆë ¨ì¥: {item['Training_Center']}"
        if item.get("transportation"):
            text += f" / êµí†µìˆ˜ë‹¨: {item['transportation']}"
        if item.get("time"):
            text += f" / {item['time']}"
        if item.get("delay"):
            text += f" / {item['delay']}"
        if item.get("description"):
            text += f" / {item['description']}"
        if item.get("print_method"):
            text += f" / {item['print_method']}"
        if item.get("refund"):
            text += f" / {item['refund']}"
        if item.get("ratio_A"):
            text += f" / {item['ratio_A']}"
        if item.get("ratio_B"):
            text += f" / {item['ratio_B']}"
        if item.get("confirm"):
            text += f" / {item['confirm']}"
        if item.get("criteria"):
            text += f" / {item['criteria']}"
        if item.get("complain"):
            text += f" / {item['complain']}"

        documents.append(text)
    return documents

def process_introduction_data():
    with open("Training_Data/introduction.json", "r", encoding="utf-8") as file:
        introduction_data = json.load(file)

    documents = []
    for item in introduction_data:
        documents.append(
            f"{item['title']} : {item['description']}")
    return documents

def process_school_data():
    with open("Training_Data/school.json", "r", encoding="utf-8") as file:
        school_data = json.load(file)

    documents = []
    for item in school_data:
        documents.append(
            f"{item['title']}\n {item['description']} / ìœ„ì¹˜: {item['location']} / ì „í™”ë²ˆí˜¸: {item['phoneNumber']} / í™ˆí˜ì´ì§€: {item['page']}")
    return documents

def process_haksa_data():
    with open("Training_Data/Haksa.json", "r", encoding="utf-8") as file:
        haksa_data = json.load(file)

    documents = []
    for item in haksa_data:
        text = f"{item['name']} - í•™ê¸°: {item['semester']} / ëŒ€ìƒ: {item['eligibility']} / ê¸°ê°„: {item['period']} / ì‹ ì²­ ë°©ë²•: {item['application']}"
        documents.append(text)
        if item.get("application"):
            text += f" / ì‹ ì²­ ë°©ë²•: {item['application']}"
    return documents

def process_major_data():
    with open("Training_Data/majors.json", "r", encoding="utf-8") as file:
        major_data = json.load(file)

    documents = []

    # ì „ê³µê³¼ëª©ê³¼ êµìˆ˜ ì •ë³´ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    for major in major_data:
        major_name = major["name"]  # ì „ê³µê³¼ëª© ì´ë¦„
        major_phone = major["phoneNumber"]  # ì „ê³µê³¼ëª© ì „í™”ë²ˆí˜¸
        major_fax = major["fax"]
        major_office = major["office"]
        major_page = major["page"]  # ì „ê³µê³¼ëª© í™ˆí˜ì´ì§€

        # ì „ê³µê³¼ëª© ì •ë³´ ì¶”ê°€
        documents.append(f"ì „ê³µ: {major_name} - ì „í™”ë²ˆí˜¸: {major_phone} / íŒ©ìŠ¤: {major_fax} / í•™ê³¼ ì‚¬ë¬´ì‹¤: {major_office} / í™ˆí˜ì´ì§€: {major_page}")

        # êµìˆ˜ ì •ë³´ ì²˜ë¦¬
        for item in major["faculty"]:
            text = f"ì „ê³µ: {major_name} - êµìˆ˜ëª…: {item['name']} / ì—°êµ¬ì‹¤: {item['lab']} / ì „í™”ë²ˆí˜¸: {item['phoneNumber']}"
            if item.get("email"):
                text += f" / ì´ë©”ì¼: {item['email']}"
            documents.append(text)

        if "curriculums" in major:
            for curriculum in major["curriculums"]:
                curriculum_name = curriculum.get("name", "Unknown Curriculum")
                department = curriculum.get("department", "N/A")
                year = curriculum.get("year", "N/A")
                semester = curriculum.get("semester", "N/A")
                completion = curriculum.get("completion", "N/A")
                credit = curriculum.get("credit", "N/A")
                text = f"ì „ê³µ: {major_name} - ê³¼ëª©ëª…: {curriculum_name} / í•™ë¶€: {department} / í•™ë…„: {year} / í•™ê¸°: {semester} / ì´ìˆ˜êµ¬ë¶„: {completion} / í•™ì : {credit}"
                documents.append(text)
    return documents

def process_student_data():
    with open("Training_Data/student.json", "r", encoding="utf-8") as file:
        student_data = json.load(file)

    documents = []

    # í•™ì  ì •ë³´ë¥¼ ì²˜ë¦¬
    for student in student_data:
        student_name = student["title"] # í•™ì 
        student_pNumber = student["phoneNumber"] # í•™ì‚¬ìš´ì˜íŒ€ ë²ˆí˜¸
        student_page = student["page"] # í•™ì  ê´€ë ¨ í˜ì´ì§€

        # í•™ì‚¬ ì •ë³´ ì¶”ê°€
        documents.append(f"{student_name} / í•™ì‚¬ìš´ì˜íŒ€ ì „í™”ë²ˆí˜¸: {student_pNumber} / í™ˆí˜ì´ì§€: {student_page}")

        # í•™ì  ì²˜ë¦¬
        for item in student["student"]:
            text = f"{item['title']}"
            # íœ´í•™
            if item["title"] == "íœ´í•™":
                text += f"ë¬¸ì˜ ì „í™”ë²ˆí˜¸: {item.get('phoneNumber')}\n"
                text += f"í™ˆí˜ì´ì§€: {item.get('page')}\n"
                text += f"ì‹ ì²­ ë°©ë²•: {item.get('method')}\n"
                text += f"ì‹ ì²­ ê¸°ê°„: {item.get('period')}\n"
                text += f"\níœ´í•™ ìœ í˜•ë³„ ì •ë³´:\n"

                for t in item.get("types", []):
                    text += f"\n- {t['type']}\n"
                    text += f"  ì„¤ëª…: {t['description']}\n"
                    text += f"  ê¸°ê°„: {t['duration']}\n"
                    if t.get("required_documents"):
                        text += f"  ì œì¶œì„œë¥˜: {t['required_documents']}\n"
                    if t.get("eligibility"):
                        text += f"  ì‹ ì²­ ìê²©: {t['eligibility']}\n"
                    if t.get("note"):
                        text += f"  ë¹„ê³ : {t['note']}\n"
            # ë³µí•™
            if item["title"] == "ë³µí•™":

                text += f"ë¬¸ì˜ ì „í™”ë²ˆí˜¸: {item.get('phoneNumber')}\n"
                text += f"í™ˆí˜ì´ì§€: {item.get('page')}\n"
                text += f"ì‹ ì²­ ë°©ë²•: {item.get('method')}\n"
                text += f"ì‹ ì²­ ê¸°ê°„: {item.get('period')}\n"

                required_docs = item.get("required_documents", {})
                if required_docs:
                    text += f"\në³µí•™ ìœ í˜•ë³„ ì œì¶œì„œë¥˜:\n"
                    if "general_leave" in required_docs:
                        text += f"- ì¼ë°˜íœ´í•™: {required_docs['general_leave']}\n"
                    if "military_leave" in required_docs:
                        text += f"- êµ°íœ´í•™: {required_docs['military_leave']}\n"
                    if "startup_leave" in required_docs:
                        text += f"- ì°½ì—…íœ´í•™: {required_docs['startup_leave']}\n"
                if item.get("notes"):
                    text += f"\nìœ ì˜ì‚¬í•­: {item['notes']}\n"
            # ì „ê³¼
            if item["title"] == "ì „ê³¼":
                text += f"ì „í™”ë²ˆí˜¸ ë¬¸ì˜: {item.get('phoneNumber')}\n"
                text += f"í™ˆí˜ì´ì§€: {item.get('page')}\n"
                text += f"ì‹ ì²­ ë°©ë²•: {item.get('method')}\n"
                text += f"ì‹ ì²­ ê¸°ê°„: {item.get('period')}\n"

                if item.get("major_transfer"):
                    text += f"\nìº í¼ìŠ¤ ë‚´ ì „ê³¼:\n- {item['major_transfer']}\n"

                if item.get("campus_transfer"):
                    text += f"\nìº í¼ìŠ¤ ê°„ ì „ê³¼:\n- {item['campus_transfer']}\n"

            # ì „ê³¼
            if item["title"] == "ì œì  ë° ìí‡´":
                text += f"ì¢…ë¥˜: {item.get('category')}\n"
                text += f"ì‹ ì²­ ë°©ë²•: {item.get('method')}\n"
                text += f"ì „í™”ë²ˆí˜¸ ë¬¸ì˜: {item.get('phoneNumber')}\n"
                text += f"í™ˆí˜ì´ì§€: {item.get('page')}\n"



            # ì¦ëª…ì„œ ë°œê¸‰
            if item["title"] == "ì¦ëª…ì„œ ë°œê¸‰":
                text += f"ë¬¸ì˜: {item.get('phoneNumber')}\n"
                text += f"í™ˆí˜ì´ì§€: {item.get('page')}\n"

            documents.append(text)

    return documents


def process_facility_data():
    with open("Training_Data/facility.json", "r", encoding="utf-8") as file:
        facility_data = json.load(file)

    documents = []

    # ìº í¼ìŠ¤ë§µ
    for facility in facility_data:
        facility_name = facility["location"]  # ìº í¼ìŠ¤ë§µ
        facility_page = facility["page"]  # ìº í¼ìŠ¤ë§µ í˜ì´ì§€

        # ì •ë³´ ì¶”ê°€
        documents.append(f"ì§€ë„: {facility_name}  / í™ˆí˜ì´ì§€: {facility_page}")

        # ì •ë³´ ì²˜ë¦¬
        for item in facility["facility"]:
            text = f"{facility_name} - {facility_page} /  {item['title']}"

            if item["title"] in ["í•™ìˆ ì •ë³´ê´€", "ë„ì„œê´€"]:
                text += " / " + " / ".join(
                    filter(None, [
                        f"íŒ©ìŠ¤: {item.get('fax')}",
                        f"ë²ˆí˜¸: {item.get('phoneNumber')}",
                        f"ìš´ì˜ì‹œê°„: {item.get('Library time')}",
                        f"ì°¸ê³ ì •ê°„ì‹¤: {item.get('reference room')}",
                        f"ì—´ëŒì‹¤ 1: {item.get('study room 1 time')}",
                        f"ì—´ëŒì‹¤ 2: {item.get('study room 2 time')}",
                        f"ë¦¬ë”©ë¼ìš´ì§€: {item.get('reading Lounge')}",
                        f"ëŒ€ì¶œ ê·œì •: {item.get('Library Loan')}",
                        f"ì—´ëŒì‹¤ ì´ìš©ì: {item.get('study room reservation user')}",
                        f"ì—´ëŒì‹¤ ì˜ˆì•½ ë°©ë²•: {item.get('study room reservation guide')}",
                        f"ì—´ëŒì‹¤ ì´ìš© ì‹œê°„: {item.get('study room reservation time')}",
                        f"ì—´ëŒì‹¤ ìë¦¬ë°°ì •ì‹œìŠ¤í…œ ìœ„ì¹˜: {item.get('study room reservation Kiosk')}",
                        f"ì„¸ë¯¸ë‚˜ì‹¤ ì˜ˆì•½: {item.get('Seminar Room reservation 1')}",
                        f"ì„¸ë¯¸ë‚˜ì‹¤ ì˜ˆì•½(ì•±): {item.get('Seminar Room reservation 2')}",
                        f"í™ˆí˜ì´ì§€: {item.get('page')}"
                    ])
                )

            if item.get("fax"):
                text += f" / íŒ©ìŠ¤: {item['fax']}"
            if item.get("phoneNumber"):
                text += f" / ë²ˆí˜¸: {item['phoneNumber']}"
            if item.get("time"):
                text += f" / ì‹œê°„: {item['time']}"
            if item.get("open"):
                text += f" / ê°œë°©: {item['open']}"
            if item.get("close"):
                text += f" / ë§ˆê°: {item['close']}"
            if item.get("location"):
                text += f" / ìœ„ì¹˜: {item['location']}"
            if item.get("email"):
                text += f" / ì´ë©”ì¼: {item['email']}"
            if item.get("page"):
                text += f" / í™ˆí˜ì´ì§€: {item['page']}"
            if item.get("study room time"):
                text += f"\n / ì—´ëŒì‹¤ ì‹œê°„: {item['study room time']}"



            documents.append(text)
    return documents
def process_double_major_data():
    with open("Training_Data/double_major.json", "r", encoding="utf-8") as file:
        double_major_data = json.load(file)

    documents = []
    for item in double_major_data:
        text = f"{item['name']} - "
        documents.append(text)

        types = item.get("type", [{}])[0]
        for t_name, t_desc in types.items():
            documents.append(f"ì¢…ë¥˜ \n{t_name}: {t_desc}")
        documents.append(
            f"ì‹ ì²­ ë°©ë²•: {item['method']} / ì‹ ì²­ ê¸°ê°„: {item['period']} / ì „í™”ë²ˆí˜¸: {item['phoneNumber']} / í™ˆí˜ì´ì§€: {item['page']}"
        )


    return documents

def process_ePortfolio_data():
    with open("Training_Data/smcareer_seoul_25.json", "r", encoding="utf-8") as file:
        ePortfolio_data = json.load(file)

    documents = []

    for item in ePortfolio_data:
        documents.append(
            f"{item['title']} - ë§í¬: {item['url']} / ê¸°ê°„: {item['period']} / ìº í¼ìŠ¤: {item['campus']}"
        )
    return documents

# JSON ë°ì´í„° ë¡œë“œ ë° ë²¡í„°í™”
def load_data_to_chroma():
    collections = {
        "scholarship": (scholarship_collection, process_scholarship_data()),
        "haksa": (haksa_collection, process_haksa_data()),
        "major": (major_collection, process_major_data()),
        "ePortfolio": (ePortfolio_collection, process_ePortfolio_data()),
        "facility": (facility_collection, process_facility_data()),
        "student": (student_collection, process_student_data()),
        "double_major" : (double_major_collection, process_double_major_data()),
        "graduation" : (graduation_collection, process_graduation_data()),
        "service" : (service_collection, process_service_data()),
        "introduction" : (introduction_collection, process_introduction_data()),
        "school" : (school_collection, process_school_data())
    }

    for category, (collection, data) in collections.items():
        if collection.count() == 0:
            print(f"{category} ë°ì´í„° ì €ì¥ ì¤‘...")
            for idx, text in enumerate(data):
                embedding = embedding_model.encode(text).tolist()
                collection.add(ids=[f"{category}_{idx}"], embeddings=[embedding], metadatas=[{"text": text}])
            print(f"{category} ë°ì´í„° ì €ì¥ ì™„ë£Œ!")

load_data_to_chroma()

def extract_professor_name(question: str):
    match = re.search(r"([ê°€-í£]+)\s*êµìˆ˜", question)
    if match:
        return match.group(1)
    return None

@app.route('/ask', methods=['POST'])
def ask():
    user_question = request.json.get("question")
    print(f"user_question : {user_question}")

    if not user_question:
        return jsonify({"error": "ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”."}), 400

    # ì‚¬ìš©ì ì§ˆë¬¸ ë²¡í„°í™” í›„ ê²€ìƒ‰
    user_embedding = embedding_model.encode(user_question).tolist()
    counts = len(user_embedding)

    haksa_results = haksa_collection.query(query_embeddings=[user_embedding], n_results=counts)
    scholarship_results = scholarship_collection.query(query_embeddings=[user_embedding], n_results=counts)
    major_results = major_collection.query(query_embeddings=[user_embedding], n_results=counts)
    ePortfolio_results = ePortfolio_collection.query(query_embeddings=[user_embedding], n_results=counts)
    facility_results = facility_collection.query(query_embeddings=[user_embedding], n_results=counts)
    student_results = student_collection.query(query_embeddings=[user_embedding], n_results=counts)
    double_major_results = double_major_collection.query(query_embeddings=[user_embedding], n_results=counts)
    graduation_results = graduation_collection.query(query_embeddings=[user_embedding], n_results=counts)
    service_results = service_collection.query(query_embeddings=[user_embedding], n_results=counts)
    introduction_results = introduction_collection.query(query_embeddings=[user_embedding], n_results=counts)
    school_results = school_collection.query(query_embeddings=[user_embedding], n_results=counts)

    haksa_docs = [doc["text"] for doc in haksa_results["metadatas"][0] if "text" in doc]
    scholarship_docs = [doc["text"] for doc in scholarship_results["metadatas"][0] if "text" in doc]
    major_docs = [doc["text"] for doc in major_results["metadatas"][0] if "text" in doc]
    ePortfolio_docs = [doc["text"] for doc in ePortfolio_results["metadatas"][0] if "text" in doc]
    facility_docs = [doc["text"] for doc in facility_results["metadatas"][0] if "text" in doc]
    student_docs = [doc["text"] for doc in student_results["metadatas"][0] if "text" in doc]
    double_major_docs = [doc["text"] for doc in double_major_results["metadatas"][0] if "text" in doc]
    graduation_docs = [doc["text"] for doc in graduation_results["metadatas"][0] if "text" in doc]
    service_docs = [doc["text"] for doc in service_results["metadatas"][0] if "text" in doc]
    introduction_docs = [doc["text"] for doc in introduction_results["metadatas"][0] if "text" in doc]
    school_docs = [doc["text"] for doc in school_results["metadatas"][0] if "text" in doc]

    prof_name = extract_professor_name(user_question)

    if prof_name:
        print(f"ğŸ” êµìˆ˜ ì´ë¦„ '{prof_name}' ìœ¼ë¡œ í•„í„°ë§ ì¤‘...")

        # ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì´ë¦„ìœ¼ë¡œ í•„í„°
        all_major_data = major_collection.get(include=["metadatas"])
        major_docs = [
            doc["text"]
            for doc in all_major_data["metadatas"]
            if "text" in doc and prof_name in doc["text"]
        ]

        if not major_docs:
            major_docs = ["í•´ë‹¹ êµìˆ˜ë‹˜ì˜ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
    else:
        # ê¸°ì¡´ ë°©ì‹: ìœ ì‚¬ë„ ê²€ìƒ‰
        major_results = major_collection.query(query_embeddings=[user_embedding], n_results=counts)
        major_docs = [doc["text"] for doc in major_results["metadatas"][0] if "text" in doc]

    for idx, doc in enumerate(facility_results["metadatas"][0]):
        print(f"ê²€ìƒ‰ ê²°ê³¼ {idx + 1}: {doc['text']}")

    for idx, doc in enumerate(student_results["metadatas"][0]):
        print(f"ê²€ìƒ‰ ê²°ê³¼ {idx + 1}: {doc['text']}")

    for idx, doc in enumerate(double_major_results["metadatas"][0]):
        print(f"ê²€ìƒ‰ ê²°ê³¼ {idx + 1}: {doc['text']}")

    for idx, doc in enumerate(graduation_results["metadatas"][0]):
        print(f"ê²€ìƒ‰ ê²°ê³¼ {idx + 1}: {doc['text']}")

    for idx, doc in enumerate(service_results["metadatas"][0]):
        print(f"ê²€ìƒ‰ ê²°ê³¼ {idx + 1}: {doc['text']}")

    haksa = "\n\n".join(haksa_docs)
    scholarship = "\n\n".join(scholarship_docs)
    major = "\n\n".join(major_docs)
    ePortfolio = "\n\n".join(ePortfolio_docs)
    facility = "\n\n".join(facility_docs)
    student = "\n\n".join(student_docs)
    double_major = "\n\n".join(double_major_docs)
    graduation = "\n\n".join(graduation_docs)
    service = "\n\n".join(service_docs)
    introduction = "\n\n".join(introduction_docs)
    school = "\n\n".join(school_docs)

    completion = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": sami_prompt},
            {"role": "system", "content": f"ë‹¤ìŒì€ í•™ì‚¬ ì¼ì • ê´€ë ¨ ì°¸ê³  ì •ë³´ì…ë‹ˆë‹¤:\n\n{haksa}"},
            {"role": "system", "content": f"ë‹¤ìŒì€ ì¥í•™ê¸ˆ ê´€ë ¨ ì°¸ê³  ì •ë³´ì…ë‹ˆë‹¤:\n\n{scholarship}"},
            {"role": "system", "content": f"ë‹¤ìŒì€ ì „ê³µ ê´€ë ¨ ì°¸ê³  ì •ë³´ì…ë‹ˆë‹¤:\n\n{major}"},
            {"role": "system", "content": f"ë‹¤ìŒì€ ìƒëª… e-í¬íŠ¸í´ë¦¬ì˜¤ ì§„ë¡œ/ì·¨ì—…í”„ë¡œê·¸ë¨ ì •ë³´ì…ë‹ˆë‹¤:\n\n{ePortfolio}"},
            {"role": "system", "content": f"ë‹¤ìŒì€ ìƒëª… ìº í¼ìŠ¤ë§µ ì •ë³´ì…ë‹ˆë‹¤:\n\n{facility}"},
            {"role": "system", "content": f"ë‹¤ìŒì€ í•™ì  ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:\n\n{student}"},
            {"role": "system", "content": f"ë‹¤ìŒì€ ì „ê³µ ì œë„ ê´€ë ¨ ì •ë³´ ì…ë‹ˆë‹¤:\n\n{double_major}"},
            {"role": "system", "content": f"ë‹¤ìŒì€ ì¡¸ì—… ìš”ê±´ ê´€ë ¨ ì •ë³´ ì…ë‹ˆë‹¤:\n\n{graduation}"},
            {"role": "system", "content": f"ë‹¤ìŒì€ í•™êµ ì„œë¹„ìŠ¤ ê´€ë ¨ ì •ë³´ ì…ë‹ˆë‹¤:\n\n{service}"},
            {"role": "system", "content": f"ë‹¤ìŒì€ ì‚¬ë¯¸(SAMI) ì±—ë´‡ ê´€ë ¨ ì •ë³´ ì…ë‹ˆë‹¤:\n\n{introduction}"},
            {"role": "system", "content": f"ë‹¤ìŒì€ ìƒëª…ëŒ€í•™êµ ê´€ë ¨ ì •ë³´ ì…ë‹ˆë‹¤:\n\n{school}"},
            {"role": "user", "content": user_question}
        ]
    )

    answer = completion.choices[0].message.content
    print(f"answer : {answer}")
    return jsonify({"answer": answer})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8000, debug=False, use_reloader=False)